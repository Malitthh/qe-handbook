# DSA - Time & Space Complexity

## Time complexity

* Time complexity is a measure of the amount of time an algorithm takes to run as a function of the size of its input. It provides an estimate of the upper bound on the running time of an algorithm, and it is expressed using big O notation (O(f(n))), where "f(n)" is a mathematical function representing the growth rate of the algorithm's running time concerning the size of the input (n).

* In simpler terms, time complexity helps us understand how the running time of an algorithm increases with the size of the input. The notation O(f(n)) is used to describe the upper limit of the growth rate, and it ignores constant factors and lower-order terms, focusing on the dominant factor that contributes most significantly to the running time as the input size becomes large.

Here are some common time complexity classes:

1. O(1): Constant time complexity. The running time of the algorithm remains constant regardless of the size of the input. An example is accessing an element in an array by index.
2. O(log n): Logarithmic time complexity. Common in algorithms that divide the input in each step, such as binary search on a sorted array.
3. O(n): Linear time complexity. The running time grows linearly with the size of the input. Examples include iterating through an array or list.
4. O(n log n): Linearithmic time complexity. Often seen in efficient sorting algorithms like merge sort and heap sort.
5. O(n^2): Quadratic time complexity. The running time is proportional to the square of the input size. Common in nested loops.
6. O(2^n): Exponential time complexity. The running time doubles with each addition to the input size. Common in brute-force algorithms.

## Space complexity

Is the amount of memory taken by an algorithm to run as a function of input size.

## Algorithms

An algorithm is a set of well-defined instructions to solve a particular problem.

* Characteristics of Algorithms
1. Well defined inputs and outputs
2. Each step should be clear
3. Language independent

The number of algorithms is vast and continually expanding as researchers and developers create new solutions to various problems. Algorithms are essentially step-by-step procedures or formulas for solving problems, and they exist in virtually every field of computer science and beyond.

Here are some broad categories of algorithms, each containing numerous specific algorithms:

1. Sorting Algorithms:

Bubble Sort
Selection Sort
Insertion Sort
Merge Sort
Quick Sort
Heap Sort
Radix Sort, etc.

2. Searching Algorithms:

Linear Search
Binary Search
Hashing Algorithms (for hash tables), etc.

3. Graph Algorithms:

Depth-First Search (DFS)
Breadth-First Search (BFS)
Dijkstra's Algorithm
Bellman-Ford Algorithm
Kruskal's Algorithm
Prim's Algorithm, etc.

4. Dynamic Programming Algorithms:

Fibonacci sequence using dynamic programming
Longest Common Subsequence
Knapsack Problem
Shortest Path in a Graph
Matrix Chain Multiplication, etc.

5. Divide and Conquer Algorithms:

Merge Sort
Quick Sort
Strassen's Matrix Multiplication, etc.

6. Greedy Algorithms:

Kruskal's Algorithm
Prim's Algorithm
Dijkstra's Algorithm
Huffman Coding, etc.

7. String Algorithms:

Pattern Matching Algorithms (e.g., KMP, Rabin-Karp)
Longest Common Substring
Edit Distance, etc.

8. Numerical Algorithms:

Euclidean Algorithm (for finding the greatest common divisor)
Fast Fourier Transform (FFT)
Primality Testing Algorithms, etc.

9. Machine Learning Algorithms:

Linear Regression
Decision Trees
Support Vector Machines
Neural Networks
K-Means Clustering, etc.

10. Cryptography Algorithms:

RSA Algorithm
AES (Advanced Encryption Standard)
Diffie-Hellman Key Exchange, etc.
These are just a few examples, and there are many more algorithms in each category. 
